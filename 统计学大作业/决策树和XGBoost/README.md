决策树能够直观地反映因子对结果的影响过程，而且一定程度上也能减少过拟合带来的危害。在这里我选了普通的决策树（Decision Tree)作为baseline，XGBoost回归和XGBoost多分类作为实验组，它们的准确率（accuracy_score）或者r2分别是0.6246648793565683、r2=0.6441016804590916、69.17%  
决策树需要对数据打上标签。根据总分的最大值、最小值和它们的差，我将总分分成了5等，对于DT它们是A,B,C,D,E;对于XGBoost它们是4，3，2，1，0（因为XGBoost要求的标签是数值）。具体打上标签的过程请参考data_handler。  
