  决策树能够直观地反映因子对结果的影响过程，而且一定程度上也能减少过拟合带来的危害。在这里我选了普通的决策树（Decision Tree)作为baseline，XGBoost回归和XGBoost多分类作为实验组，它们的准确率（accuracy_score）或者r2分别是0.6246648793565683、r2=0.6441016804590916、69.17%  
  决策树需要对数据打上标签。根据总分的最大值、最小值和它们的差，我将总分分成了5等，对于DT它们是A,B,C,D,E;对于XGBoost它们是4，3，2，1，0（因为XGBoost要求的标签是数值）。具体打上标签的过程请参考data_handler。  
  另外，我还统计了feature_importances_。结果显示XGBoost回归和多分类对因子的重要性判断略有出入，但都认为f9（学校对于创新创业的教育）是最重要的；而f0（是否为重点大学）和f2(性别）对模型的重要程度最小。
